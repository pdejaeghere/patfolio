---
title: Faut-il craindre l'IA?
layout: default 
nav_order: 2
parent: IA
---

# Faut-il craindre l'IA?


*Démystifier l'IA, ou plutôt « démythifier » une IA rongée par les mythes (Terminator, L'Odyssée de l'espace)....* voila ce que propose Cedric Villani dans cette vidéo très padagogique parfaitement construite [Conférence de Cedric Villani](https://www.youtube.com/watch?v=lASY63CLdSE&t=917s). 

Voici ce que j'ai retenu de cette brillante confèrence

*Déjà, il existe une difficulté à définir rigoureusement l’IA* : à part « quelque chose qui fait le buzz » (actuellement ce sont les LLM, mais hier — il n’y a pas si longtemps, 5 ou 10 ans — c’était la classification d’image, la prédiction du prix d’une maison, la recommandation d’un film, d’un ‘post’ sur les réseaux sociaux, ou d’un article selon les achats récents… même la vision par ordinateur n’impressionne déjà plus vraiment !).  
Plus sérieusement, on pourrait généraliser l’IA à tout algorithme qui automatise ce que fait l’humain pour une tâche donnée (un simple tri, une recherche dans une liste, une multiplication…). Et il n’y a pas besoin d’ordinateur pour ça : les premiers algorithmes datent d’il y a 4000 ans.

Bien entendu, on ne parlera d’IA que si l’algorithme présente une relative autonomie, notamment vis-à-vis de son développeur humain, en étant capable de s’adapter et d’apprendre comme peut le faire un être vivant. Les systèmes experts des années 90 pouvaient simuler notre capacité logique pour arriver à des conclusions en partant de faits et en appliquant des règles de causalité complexe et non ordonnée, mais ces règles étaient maîtrisées et saisies par des experts humains. L’apprentissage guidé ou exploratoire est d’une tout autre dimension. À partir d’un grand nombre d’exemples existants, labellisés encore par des humains (apprentissage supervisé), ou en devinant les classifications (non supervisé), ou, mieux encore, en explorant un ensemble de possibilités (renforcement), on se rapproche davantage du mythe de la machine humanoïde.

Mais ce qui fait partie de la fascination, ou de la grande peur — le fameux mythe de Pygmalion — c’est la branche « deep learning », sous-branche du machine learning citée précédemment, car apparaît, pour la décrire, le nom très évocateur de « réseau de neurones » (profonds ou convolutifs) qui, bien évidemment, s'associe directement au fonctionnement du cerveau et donc de l’intelligence humaine.

Mais s’agit-il pour autant d’intelligence humaine ? Certainement non ! Même si c’est d’une efficacité bluffante, capable d’écraser l’homme au jeu d’échecs ou au go, de reconnaître des visages, la voix… tout cela rendu possible par cette famille d’algorithmes, très ancienne dans ses principes, mais repropulsée sur le devant de la scène par les capacités hardware (CPU, GPU, mémoire) qui n’ont fait qu’augmenter selon la loi de Moore depuis le début de l'informatique et qui, accessoirement, fait la fortune actuelle de Nvidia.

Encore plus bluffant, et conférant à l'antropomorphisme de nos machines, sont arrivés ChatGPT et consort, toujours sur la base de réseaux neuronaux et d'un nouveau terme plutot mysterieux: les transformers, capable de comprendre le langage, de dialoguer jusqu'à réussire quasiment le test de turing, de faire des résumés, des traductions, de la création et même de realiser des actions sur nos machines,  après avoir ingurgité pendant des heures et des heures le corpus de la connaissance humaine (merci le web),sans oublier cette capacité de créer des images , des vidéos, de la musique... La peur commence à gagner l'humanité, parfois à grand renfort d'annonces des nouveaux milliardaires à la tête de ces algos necessitant toujours plus d'investissement, tellement ces technologies sont gourmandes en infrastructures de calculs. 

Alors danger pour l'humanité? 

On lit tout et son contraire... Pour certains, ce ne sont que des supers outils de complétion à base de statistique et de probabilité. Pour d'autres, il y a un mystère proche d'une singularité, une émergence de quelque chose qui échappe à notre maîtrise, le tout étant supérieur à la somme des parties comme en biologie ... Dans [cette vidéo de  David Louapre](https://www.youtube.com/watch?v=YcIbZGTRMjI&t=343s), le brillant vulgarisateur nous explique ainsi que la création humaine peut s'apparenter à une sorte d'interpolation au même titre que ce que font les algorithmes de machine learning pour prédire une valeur à partir d'entrée non rencontrées (non apprises). Donc, ne soyons pas trop prétencieux à mettre notre intelligence et créativité au dessus de tout...

Sommes-nous donc en train de vivre une révolution copernicienne quand il a bien fallu se rendre compte que l'homme n'est pas au centre de l'univers? À défaut de trouver une intelligence ailleurs, se l'est-il inventée? Hasard ou nécessité avant de disparaître, anéanti par sa propre création et les problèmes écologiques que cette invention semble accélerer dans cette course concurrentielle, totallement folle, entre les GAFAs? - Lire à ce propos cet [article de Jean-Marc Jeancovici](https://www.linkedin.com/posts/jean-marc-jancovici_le-d%C3%A9veloppement-actuel-des-centres-de-donn%C3%A9es-activity-7379407499344662528-m4HY?utm_source=share&utm_medium=member_desktop&rcm=ACoAAEg-s-8Bx0HCCg0W8i4G8cqldBH9ix7CMho) et [le rapport du Shift projet](https://theshiftproject.org/app/uploads/2025/09/Synthese-RF-PIA-1.pdf) - 

Stoppons ces quelques questions philosophiques et surtout ces scénarios catastrophes, on en est pas encore là. Ecoutons plutôt Etienne Kien qui apporte un joli contre point 
dans la conversation suivante [La science est-elle menacée par l'IA ? Étienne Klein répond](https://www.youtube.com/watch?v=MRRIhM4ih-g) où il explique que les grandes révolutions dans la physique (Gallilé, Newton, Enstein, Bohr ) se sont faites par des experiences de pensée, des fulgurances de l'esprit qui n'avaient pas à disposition des milliards de données,et ça, pour le moment - il reste prudent comme tout grand scientifique- l'IA en est encore bien incapable.

Mais il y a quand même un énorme risque avec l'IA générative, c'est de l'utiliser sans apprendre, sans progresser, sans s'approprier. C'est le problème actuel à l'école. Les éléves ne font plus l'effort (l'humain est paresseux par nature et cherche toujours le plus court chemin). Et à force d'utiliser cette bequille, et bien comment pouvoir s'en passer? comment s'améliorer? et en admettant que ça permette de faire autre chose, mais quoi donc? C'est ce que dénonce un informaticien que je suis sur linkedin depuis peu car j'aime bien son esprit de révolte dans ce verbiage technophille pro IA qui innonde le réseau avec des invectives sans rémission "si tu n'utilises pas l'IA, tu es has been, mort, balayé par les autres qui feront tout mieux et plus vite"
[L'IA générative : L'ère du vide artificiel - Pierre Vannier](https://www.linkedin.com/pulse/lia-g%25C3%25A9n%25C3%25A9rative-l%25C3%25A8re-du-vide-artificiel-pierre-vannier-h0uqe/?trackingId=VTFbHkyDRq%2Ba8jXmQGtOZA%3D%3D)

Alors, oui, utilisons l'IA, comme on utilise le web pour rechercher une solution à un problème déjà rencontré, mais ne perdons pas le goût de l'effort, ne soyons pas fier d'avoir fait travailler l'IA à notre place. Courrir 5, 10, 20, 40 km,  ça demande un effort, idem , apprendre à jouer d'un instrument de musique.. et cet effort, dans le sport, est une source de plaisir, de satisfaction, même si ce n'est pas simple de s'y  mettre ou remettre. 
