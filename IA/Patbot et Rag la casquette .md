---
title: Patbot et Rag la casquette!
layout: default 
nav_order:  5
parent: IA
---

# Patbot et Rag la casquette !

J'expliquais dans l'article [mamma mia](mamma mia.html) le travail et le b√©n√©fice d'avoir mis en place un chatbot maison enrichi par une base de donn√©es RAG.  Je propose d'apporter quelques d√©tails, sachant qu'il a √©t√© d√©velopp√© en c# avec le framework SemanticKernel mais que je suis en train de r√©√©crire un √©quivalent pour un usage personnel et pour qui n'en veut üòâ en utilisant le rempla√ßant de SemanticKernel: Microsoft Agent Framework et Avalonia (WPF multi-plateforme).

Mais revenons au travail initial... 

**Tout d‚Äôabord, j‚Äôai compris le point le plus important** : toute la conversation est envoy√©e syst√©matiquement au LLM ; il n‚Äôa aucune m√©moire li√© √† une session quelconque. Si, √† chaque question, on n‚Äôenvoie pas ce qui s‚Äôest dit auparavant, la conversation devient tr√®s vite un dialogue de sourds. Le second point est la possibilit√© de mettre ce qu‚Äôon veut, par programmation j‚Äôentends, dans le contexte avant de l‚Äôenvoyer. C‚Äôest ainsi qu‚Äôon peut compl√©ter le prompt, donner des instructions de r√©ponse, ajouter de la connaissance sans que l‚Äôutilisateur ne le sache ni ne le voit. Par contre, c‚Äôest l√† que les LLM peuvent montrer des limitations indiqu√©es par le nombre de tokens accept√©s et que le d√©veloppeur peut user d‚Äôastuces pour tenter de compresser au maximum ce contexte (par exemple, en utilisant pr√©alablement une question au LLM qui restera cach√© pour l'utilisateur: ¬´ Fais-moi un r√©sum√© du contexte en X mots ¬ª) ou retirer les parties les plus anciennes.

Ensuite, avec cette notion de **plugin**, on dispose du moyen de transformer le chatbot en agent capable de r√©aliser des actions (par exemple : lire ou cr√©er un fichier, rechercher dans une base de donn√©es, dans un RAG, sur Internet, envoyer un mail, etc.). Des fonctions sont cod√©es et inject√©es via le framework dans le contexte du prompt (non visible pour l‚Äôutilisateur) √† travers une description pr√©cise et courte (fourni dans un m√©ta-attribut de la fonction ou plut√¥t m√©thode en C#). Sur cette base, certains LLMs (pas tous, a priori) ont √©t√© entra√Æn√©s pour g√©n√©rer une r√©ponse contenant une codification pr√©cise correspondant √† une des fonctions‚ÄØ: celle-ci est intercept√©e par le framework Semantic Kernel, qui appellera alors la fonction en question. Il faut √™tre tr√®s vigilant √† d√©finir les fonctions de mani√®re unique (pas seulement le nom, mais aussi la s√©mantique de la description) pour √©viter les t√©lescopages (une fonction appel√©e √† la place d‚Äôune autre).

Pour un d√©veloppeur, voil√† une belle opportunit√© d‚Äôenrichir l‚Äôinterface homme-machine de son logiciel, qui pourra effectuer des op√©rations pilot√©es par des instructions en langage naturel (et m√™me par la voix, si on utilise un syst√®me de reconnaissance vocale).  
Une autre possibilit√© consiste √† mettre en place des multi-agents, des microservices recevant des instructions, op√©rant un certain nombre d‚Äôactions puis envoyant le r√©sultat √† un autre agent. Voil√† de quoi mettre en place des architectures selon les patterns √©prouv√©s de faible couplage et forte coh√©sion, bas√©es sur des dialogues et de l‚Äôenrichissement de contexte inter-agent.  

Petite parenth√®se, dans l‚Äôesprit des plugins Semantic Kernel mais d√©passant l‚Äô√©cosyst√®me .Net, un protocole standard a √©t√© introduit‚ÄØ: MCP (Model Context Protocol), qui permet d‚Äôenrichir les actions des LLMs en appelant des serveurs MCP (un programme qui est lanc√© et qui renvoit du texte sur la sortie standard)) fournissant des informations qui vont enrichir le contexte, puis relancer le travail du LLM sur la base de ce nouveau contexte.

**Mais la vraie plus-value de ce chatbot a √©t√© la mise en place d‚Äôun RAG**. Le RAG, c‚Äôest une base de donn√©es de texte issus de documents (convertis en texte), que l‚Äôon d√©coupe en morceaux (ni trop petits, ni trop grands, c‚Äôest toute la difficult√©). Chaque morceau est transform√© en un vecteur s√©mantique (embedding vector, obtenu via un r√©seau de neuronne propre aux LLMs nomm√© *transformer*). Quand une question est pos√©e, on la transforme de la m√™me mani√®re en vecteur s√©mantique . Ensuite, avec une op√©ration math√©matique assez triviale (calcul de similarit√© cosinus), on calcule la proximit√© de ce vecteur "prompt" avec tout ceux du RAG (ce qui revient √† rechercher les morceaux s√©mantiquement proches, donc en lien avec le sujet de la question, et ceci ind√©pendamment de la langue!). 

Le gros int√™ret du Rag est qu'on n'a pas besoin de 'fine-tuner' nous m√™me un mod√®le LLM, ce qui demande des milliers d'heures de calcul.
On se contente d'ajouter des morceaux de texte au contexte du prompt. Bien entendu, il faut mettre en place une heuristique pour √©viter de submerger le contexte de trop d'information (on peut d√©finir un niveau de pertinance en jouant sur le coefficient de similarit√© des vecteurs, on peut limiter le nombre de morceau trouv√©, etc..), c'est une des limitations par rapport √† un mod√®le "fine-tun√©" √† partir d'un corpus de documents s√©pcifiques.

Un aspect concluant a √©t√© d‚Äôajouter le nom de la source (et la page, pour des documents pdf ou office) √† chaque morceau de texte, offrant ainsi √† l‚Äôutilisateur la possibilit√© d‚Äôouvrir le document √† la bonne page √† sa demande aupr√®s du LLM via ... un plugin d√©di√© : opendocument(source,page) !

Le plus fort, c'est que la recherche dans le RAG elle m√™me √©tait faite via un pluggin. On trouve beaucoup d'exemple qui parte de la question de l'utilisateur pour aller rechercher les morceaux en lien dans le RAG .Mais si la question est juste "Combien font 2 +2", c'est perte de temps et de ressources. Par contre, via une fonction bien d√©crite, "SearchInRaG( summaryOfContext )" en endiquant le th√®me du RAG,  on laisse le LLM le choix ou non d'aller rechercher de l'information.  Le r√©sultat a √©t√© tr√®s satisfaisant m√™me si parfois il faut que l'utilisateur se montre un peu insistant pour que la recherche se d√©clenche - ex: "tu es sur d'√™tre sur de ne rien trouver ?" üòÑ -. 


*Le chatbot mis en place est donc une application .NET utilisant une base de donn√©es et un service LLM distant sur Azure (ou en local). Nous avons ensuite mis en place diff√©rentes strat√©gies de d√©ploiement : local (mono-utilisateur), web (multi-utilisateur), base de donn√©es locale ou distante, d√©ploiement local ou distant via conteneur Docker, etc.  toutes ses probl√®matiques d'architecture en fonction des besoins utilisateurs restant classique et, surtout, n'ayant rien √† voir avec l'IA.*

